# -*- coding: utf-8 -*-
"""sentiment_analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uzqwSoK8QU1JPmw7UykPX9oxBJ7VvIY0
"""

import pandas as pd
import numpy as np
import nltk
import matplotlib.pyplot as plt
import seaborn as sns
from nltk.stem import WordNetLemmatizer
from gensim.parsing.preprocessing import remove_stopwords

df=pd.read_csv("/content/train.csv (3).zip",encoding= 'unicode_escape')

df

df.isnull().sum()

df=df[['text','sentiment']]
df

df.isnull().sum()

df['text'] = df['text'].fillna(' ')

df.isnull().sum()

df.shape

df['sentiment'].value_counts().index

"""Exploratory Data Analysis (EDA)"""

sns.histplot(df['sentiment'],kde=True)

plt.figure(figsize=(5,5))
plt.title('Percentage of sentiment', fontsize=20)
df.sentiment.value_counts().plot(kind='pie', labels=['neutral', 'positive','negative'], autopct='%2.f')

x= df['sentiment'].value_counts()
y= x.sort_index()
plt.figure(figsize=(5,5))
sns.barplot(x=x.index,y=x.values, alpha=0.8)
plt.title("sentiment Distribution")
plt.ylabel('Frequency')
plt.xlabel('sentiment')

df['text']=df['text'].map(str)

df['sentiment'].replace(['negative','neutral','positive'],[-1,0,1],inplace = True)

df

import string
punctuations=string.punctuation
def clean_punctuations(tweet):
    translator = str.maketrans('','',punctuations)
    return tweet.translate(translator)
df['text'] = df['text'].apply(lambda i: clean_punctuations(i))
df

"""STOP_WORDS_REMOVAL"""

def remove_stop_word(text):
  words = remove_stopwords(text)
  return words
df['text'] = df['text'].apply(lambda i: remove_stop_word(i))

df

""" Tokenization"""

from gensim.utils import simple_preprocess
df['tokenized_text'] = [simple_preprocess(line, deacc=True) for line in df['text']]
df

"""APPLYING STEMMING"""

from gensim.parsing.porter import PorterStemmer
porter_stemmer = PorterStemmer()
df['stemmed_tokens'] = [[porter_stemmer.stem(word) for word in tokens] for tokens in df['tokenized_text'] ]
df

# import re
# corpus = []
# for i in range(0, 27481):
#     review = re.sub(r'\W', ' ', str(df["text"][i]))
#     review = review.lower()
#     review = re.sub(r'^br$', ' ', review)
#     review = re.sub(r'\s+br\s+',' ',review)
#     review = re.sub(r'\s+[a-z]\s+', ' ',review)
#     review = re.sub(r'^b\s+', '', review)
#     review = re.sub(r'\s+', ' ', review)
#     corpus.append(review)

# # Creating the BOW model
# from nltk.corpus import stopwords
# from sklearn.feature_extraction.text import CountVectorizer
# vectorizer = CountVectorizer(max_features = 27481 , min_df = 3, max_df = 0.6, stop_words = stopwords.words('english'))
# count_vect = vectorizer.fit_transform(corpus).toarray()

#PLOTING WORD_CLOUD FOR NEUTRAL WORD
from wordcloud import WordCloud
import matplotlib.pyplot as plt
pos=' '.join([i for i in df['text'][df['sentiment']==0]])
plt.figure(figsize = (20,20))
wc=WordCloud(max_words = 1000 , width = 1600 , height = 800,collocations=False).generate(pos)
plt.imshow(wc)

#PLOTING WORD_CLOUD FOR POSITIVE WORD
from wordcloud import WordCloud
import matplotlib.pyplot as plt
pos=' '.join([i for i in df['text'][df['sentiment']==1]])
plt.figure(figsize = (20,20))
wc=WordCloud(max_words = 1000 , width = 1600 , height = 800,collocations=False).generate(pos)
plt.imshow(wc)

#PLOTING WORD_CLOUD FOR negative WORD
from wordcloud import WordCloud
import matplotlib.pyplot as plt
pos=' '.join([i for i in df['text'][df['sentiment']==-1]])
plt.figure(figsize = (20,20))
wc=WordCloud(max_words = 1000 , width = 1600 , height = 800,collocations=False).generate(pos)
plt.imshow(wc)

df['stemmed_tokens'] = [','.join(map(str, i)) for i in df['stemmed_tokens']]
df

import re
def remove_comma(text):
  return re.sub(',',' ',text)
df['stemmed_tokens'] = df['stemmed_tokens'].apply(lambda i: remove_comma(i))

df

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(df['stemmed_tokens'], df['sentiment'], test_size = 0.3, random_state = 0)
print("Value counts for Train sentiments")
print(y_train.value_counts())
print("Value counts for Test sentiments")
print(y_test.value_counts())

print(x_train)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)

from sklearn.feature_extraction.text import CountVectorizer
from gensim.models import Word2Vec

# y=df['sentiment']

# from sklearn.model_selection import train_test_split
# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 0)

cv = CountVectorizer()
cv.fit(x_train)
x_train=cv.transform(x_train)
x_test = cv.transform(x_test)

#APPLYING LogisticRegression MODEL
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score,accuracy_score
model_1=LogisticRegression()
model_1.fit(x_train,y_train)
pred_1=model_1.predict(x_test)
a1=accuracy_score(y_test,pred_1)

a1

#APPLYING SVM MODEL
from sklearn.svm import LinearSVC
model_2=LinearSVC()
model_2.fit(x_train,y_train)
pred_2=model_2.predict(x_test)
a2=accuracy_score(y_test,pred_2)

a2

#APPLYING NAIVE_BYAS
from sklearn.naive_bayes import BernoulliNB
model_3=BernoulliNB()
model_3.fit(x_train, y_train)
pred_3= model_3.predict(x_test)
a3=accuracy_score(y_test,pred_3)

a3

from sklearn import decomposition, ensemble
from sklearn.metrics import confusion_matrix,classification_report
model_4=ensemble.RandomForestClassifier()
model_4.fit(x_train,y_train)
pred_4 = model_4.predict(x_test)
a4=accuracy_score(y_test,pred_4)
print(a4)
y_pred = model_4.predict(x_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
print(classification_report(y_test,y_pred))

